{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sesi√≥n 3: Procesamiento Avanzado de Im√°genes** ‚öôÔ∏èüñºÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Instalaciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\javie\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\javie\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\javie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/301.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 301.8/301.8 kB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.1\n",
      "    Uninstalling joblib-1.1.1:\n",
      "      Successfully uninstalled joblib-1.1.1\n",
      "Successfully installed joblib-1.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.4.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm scikit-learn openpyxl pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Librer√≠as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from bow import BoW\n",
    "from dataset import Dataset\n",
    "from image_classifier import ImageClassifier\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Apartado C**: Detecci√≥n de puntos de inter√©s y Bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C1: Detecci√≥n de puntos de inter√©s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1.1**: Generar funci√≥n de filtro gaussiano\n",
    "\n",
    "Se generar√° a partir de la imagen base las im√°genes con el filtrado gaussiano. Para ello se emplear√° la funci√≥n `generateGaussianImages()` que tiene que desarrollar empleando la funci√≥n [`cv2.GaussianBlur()`](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gae8bdcd9154ed5ca3cbc1766d960f45c1). Es recomendble visualizar los resultados para comprobar el funcionamiento. Para ello convierta los datos de la imagen a visualizar a `np.uint8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"partC\"\n",
    "folder_path = os.path.join(os.path.dirname(os.getcwd()), folder_name)\n",
    "os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_paths = sorted(glob.glob(\"../data/source/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGaussianImages(image, sigmas):\n",
    "    \"\"\"Generate the gaussian images using the base image and the sigmas given\n",
    "\n",
    "    Args:\n",
    "        image (np.array[np.float32]): Base image to blur\n",
    "        sigmas (List[np.float32]): Sigmas for blurring the image \n",
    "\n",
    "    Returns:\n",
    "        List[np.array[np.float32]: List of blurred images\n",
    "    \"\"\"\n",
    "    gaussian_images = [image]\n",
    "\n",
    "    # Generate blurred images for each sigma\n",
    "    for sigma in sigmas:\n",
    "        # Apply Gaussian blur with the current sigma value\n",
    "        blurred_image = cv2.GaussianBlur(gaussian_images[-1], (0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "        gaussian_images.append(blurred_image)\n",
    "    \n",
    "    return gaussian_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1.2**: Generaci√≥n de espacio de escalas con im√°genes gaussianas\n",
    "\n",
    "Empleando la funci√≥n anterior, complete la siguiente celda para generar las im√°genes gaussianas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the image and convert to float32\n",
    "image = cv2.imread(source_paths[0])  # Replace \"path_to_image\" with the actual file path\n",
    "image = image.astype(np.float32)\n",
    "\n",
    "# Number of Difference of Gaussians to generate\n",
    "intervals = 5  # Adjust this number if needed for more keypoints\n",
    "\n",
    "# Initial sigma value\n",
    "sigma = 1.6\n",
    "\n",
    "# Generate the necessary sigmas\n",
    "def generateGaussianSigmas(sigma, intervals):\n",
    "    \"\"\"Generate a list of sigmas for Gaussian blur at each level.\"\"\"\n",
    "    sigmas = [sigma * (np.sqrt(2) ** i) for i in range(intervals + 1)]\n",
    "    return sigmas\n",
    "\n",
    "# Get the sigma values\n",
    "sigmas = generateGaussianSigmas(sigma, intervals)\n",
    "\n",
    "# Generate the Gaussian images using the generated sigmas\n",
    "gaussian_images = generateGaussianImages(image, sigmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1.3**: Generaci√≥n de diferencias de gaussianas\n",
    "\n",
    "Utilizando la lista de im√°genes gaussianas se generar√° una lista con las diferencias entre pares consegutivos. Para ello se emplear√° la funci√≥n `generateDoGImages()` que tiene que desarrollar empleando la funci√≥n [`cv2.subtract()`](https://docs.opencv.org/4.x/d2/de8/group__core__array.html#gaa0f00d98b4b5edeaeb7b8333b2de353b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDoGImages(gaussian_images):\n",
    "    \"\"\"Generate Difference-of-Gaussians list\n",
    "\n",
    "    Args:\n",
    "        gaussian_images (List[np.array[np.float32]): List of blurred images\n",
    "\n",
    "    Returns:\n",
    "        List[np.array[np.float32]: List of difference of gaussian images\n",
    "    \"\"\"\n",
    "    dog_images = []\n",
    "\n",
    "    # Generate the list of Difference-of-Gaussians\n",
    "    for i in range(1, len(gaussian_images)):\n",
    "        # Compute the difference between consecutive Gaussian images\n",
    "        dog_img = cv2.subtract(gaussian_images[i], gaussian_images[i - 1])\n",
    "        dog_images.append(dog_img)\n",
    "    \n",
    "    return dog_images\n",
    "\n",
    "dog_images = generateDoGImages(gaussian_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1.4**: Evaluaci√≥n de extremos\n",
    "\n",
    "La funci√≥n `isPixelAnExtremum()` debe evaluar si el pixel central del bloque compuesto por las regiones de las 3 im√°genes es un m√°ximo o m√≠nimo entre ellos. Como prevenci√≥n, tenga en cuenta que puede tomar valores positivos y negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPixelAnExtremum(first_subimage, second_subimage, third_subimage, threshold):\n",
    "    \"\"\"Return True if the center element of the 3x3x3 array composed of subimages is strictly greater than or less than all its neighbors, False otherwise\n",
    "\n",
    "    Args:\n",
    "        first_subimage (np.array): Patch from first gaussian\n",
    "        second_subimage (np.array): Patch from second gaussian\n",
    "        third_subimage (np.array): Patch from third gaussian\n",
    "        threshold (float): Value threshold for the pixel\n",
    "\n",
    "    Returns:\n",
    "        Bool: True if maximum or minimum, False otherwise\n",
    "    \"\"\"\n",
    "    # Center pixel value in the middle subimage\n",
    "    center_value = second_subimage[1, 1]\n",
    "\n",
    "    # Combine all pixels in the 3x3x3 neighborhood excluding the center pixel\n",
    "    neighbors = np.concatenate([\n",
    "        first_subimage.flatten(),\n",
    "        np.delete(second_subimage.flatten(), 4),  # Exclude center of second_subimage\n",
    "        third_subimage.flatten()\n",
    "    ])\n",
    "\n",
    "    # Extract maximum and minimum values from neighbors\n",
    "    neighbors_max = neighbors.max()\n",
    "    neighbors_min = neighbors.min()\n",
    "\n",
    "    # Check if center pixel is a maximum or minimum and meets the threshold\n",
    "    if (center_value > neighbors_max and center_value > threshold) or \\\n",
    "       (center_value < neighbors_min and abs(center_value) > threshold):\n",
    "        extremum = True\n",
    "    else:\n",
    "        extremum = False\n",
    "\n",
    "    return extremum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_subimage = np.zeros((3,3),np.float32)\n",
    "third_subimage = np.zeros((3,3),np.float32)\n",
    "second_subimage = np.ones((3,3),np.float32)\n",
    "isPixelAnExtremum(first_subimage,second_subimage, third_subimage, 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1.5**: Localizaci√≥n de puntos clave y orientaci√≥n de los mismos\n",
    "\n",
    "La funci√≥n `findScaleSpaceExtrema()` obtendr√° los puntos clave y su orientaci√≥n a partir de las gaussianas y sus diferencias empleando, entre otras, la funci√≥n `isPixelAnExtremum()`. Deber√° completar las partes indicadas para recorrer todos los trios de im√°genes consecutivos y evaluar los p√≠xeles necesarios de estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Keypoint Detection with Taylor Expansion and Contrast Thresholding\u001b[39;00m\n\u001b[0;32m     34\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m\n\u001b[1;32m---> 35\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mfindScaleSpaceExtrema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgaussian_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdog_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m visualizeKp(image, keypoints)\n\u001b[0;32m     37\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m removeDuplicateKeypoints(keypoints)\n",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m, in \u001b[0;36mfindScaleSpaceExtrema\u001b[1;34m(gaussian_images, dog_images, num_intervals, sigma, threshold)\u001b[0m\n\u001b[0;32m     17\u001b[0m third_subimage \u001b[38;5;241m=\u001b[39m third_image[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Check if the center pixel is an extremum\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43misPixelAnExtremum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_subimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_subimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthird_subimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Refine the keypoint localization\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     localization_result \u001b[38;5;241m=\u001b[39m localizeExtremumViaQuadraticFit(i, j, image_index, num_intervals, dog_images, sigma)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m localization_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[32], line 28\u001b[0m, in \u001b[0;36misPixelAnExtremum\u001b[1;34m(first_subimage, second_subimage, third_subimage, threshold)\u001b[0m\n\u001b[0;32m     25\u001b[0m neighbors_min \u001b[38;5;241m=\u001b[39m neighbors\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Check if center pixel is a maximum or minimum and meets the threshold\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (center_value \u001b[38;5;241m>\u001b[39m neighbors_max \u001b[38;5;129;01mand\u001b[39;00m center_value \u001b[38;5;241m>\u001b[39m threshold) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m     29\u001b[0m    (center_value \u001b[38;5;241m<\u001b[39m neighbors_min \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(center_value) \u001b[38;5;241m>\u001b[39m threshold):\n\u001b[0;32m     30\u001b[0m     extremum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, threshold=0.03):\n",
    "    \"\"\"Find pixel positions of all scale-space extrema in the image pyramid.\"\"\"\n",
    "    keypoints = []\n",
    "\n",
    "    # Loop through the DoG images, excluding the first and last to avoid boundary issues\n",
    "    for image_index in range(1, len(dog_images) - 1):\n",
    "        first_image = dog_images[image_index - 1]\n",
    "        second_image = dog_images[image_index]\n",
    "        third_image = dog_images[image_index + 1]\n",
    "\n",
    "        # Move the 3x3 window across the whole image, skipping the borders\n",
    "        for i in range(1, second_image.shape[0] - 1):\n",
    "            for j in range(1, second_image.shape[1] - 1):\n",
    "                # Extract 3x3 patches for each image\n",
    "                first_subimage = first_image[i-1:i+2, j-1:j+2]\n",
    "                second_subimage = second_image[i-1:i+2, j-1:j+2]\n",
    "                third_subimage = third_image[i-1:i+2, j-1:j+2]\n",
    "\n",
    "                # Check if the center pixel is an extremum\n",
    "                if isPixelAnExtremum(first_subimage, second_subimage, third_subimage, threshold):\n",
    "                    # Refine the keypoint localization\n",
    "                    localization_result = localizeExtremumViaQuadraticFit(i, j, image_index, num_intervals, dog_images, sigma)\n",
    "                    \n",
    "                    if localization_result is not None:\n",
    "                        keypoint, localized_image_index = localization_result\n",
    "                        # Get the keypoint orientation(s)\n",
    "                        keypoints_with_orientations = computeKeypointsWithOrientations(keypoint, gaussian_images[localized_image_index])\n",
    "                        for keypoint_with_orientation in keypoints_with_orientations:\n",
    "                            keypoints.append(keypoint_with_orientation)\n",
    "                            \n",
    "    return keypoints\n",
    "\n",
    "# Keypoint Detection with Taylor Expansion and Contrast Thresholding\n",
    "threshold = 0.03\n",
    "keypoints = findScaleSpaceExtrema(gaussian_images, dog_images, intervals, sigma, threshold)\n",
    "visualizeKp(image, keypoints)\n",
    "keypoints = removeDuplicateKeypoints(keypoints)\n",
    "visualizeKp(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los puntos clave totalmente definidos y las im√°genes gaussianas ya podemos obtener los descriptores con la funci√≥n proporcionada de `generateDescriptors()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = generateDescriptors(keypoints, gaussian_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1.6**: Pipeline de generaci√≥n de puntos clave y descriptores\n",
    "\n",
    "Complete la funci√≥n con los m√©todos desarrollados en tareas previas para generar los puntos clave y descriptores necesarios dada una imagen, un valor de sigma y un n√∫mero de diferencias de gaussianas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeKeypointsAndDescriptors(image, sigma=1.6, num_intervals=3):\n",
    "    \"\"\"Compute SIFT keypoints and descriptors for an input image \"\"\"\n",
    "    # TODO: Fill the pipeline to get the keypoint and descriptors as before\n",
    "    image = image.astype('float32')\n",
    "    gaussian_kernels = None\n",
    "    gaussian_images = None\n",
    "    dog_images = None\n",
    "    keypoints = None\n",
    "    descriptors = None\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1.7**: Correspondencia de caracter√≠sticas entre im√°genes\n",
    "\n",
    "Cargue las im√°genes con `cv2` y complete la llamada a la funci√≥n previa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the images with cv2 in grayscale\n",
    "img1 = None\n",
    "img2 = None\n",
    "\n",
    "# TODO: Fill the function calls\n",
    "kp1, des1 = computeKeypointsAndDescriptors()\n",
    "kp2, des2 = computeKeypointsAndDescriptors()\n",
    "\n",
    "matchFeatures(img1, kp1, des1, img2, kp2, des2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ampliaci√≥n**: SIFT con OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_0 = cv2.imread('../data/0.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image_1 = cv2.imread('../data/0_rot.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create a SIFT detector object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints_0, descriptors_0 = sift.detectAndCompute(image_0, None)\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(image_1, None)\n",
    "\n",
    "# Draw keypoints on the image\n",
    "image_with_keypoints_0 = cv2.drawKeypoints(image_0, keypoints_0, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "image_with_keypoints_1 = cv2.drawKeypoints(image_1, keypoints_1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.imshow(image_with_keypoints_0)\n",
    "plt.title('Keypoints with Blob Sizes and Orientations')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.imshow(image_with_keypoints_1)\n",
    "plt.title('Keypoints with Blob Sizes and Orientations')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Initialize BFMatcher\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "# Perform KNN matching with k=2 for ratio test\n",
    "matches = bf.match(descriptors_0, descriptors_1)\n",
    "\n",
    "# sort matches by distance\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "\n",
    "# Draw the good matches\n",
    "matched_image = cv2.drawMatches(image_0, keypoints_0, image_1, keypoints_1, matches[:50], None,\n",
    "                                flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display the matched features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(matched_image)\n",
    "plt.title(\"Feature Matches\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pregunta C1**: Correspodencia de im√°genes propias y evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C2: Bolsa de palabras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2.1**: Carga de los datasets de entrenamiento y validaci√≥n para la bolsa de palabras\n",
    "\n",
    "Empleando el m√©todo `load()` de la clase [`Dataset`](.\\dataset.py) que se le proporciona, cargue los datasets de entrenamiento y validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset.load()\n",
    "validation_set = Dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2.2**: Extracci√≥n de los descriptores\n",
    "\n",
    "Para poder crear la bolsa de palabras se va a comenzar obteniendo los descriptores que ser√°n agrupados en las distintas palabras de nuestra bolsa. Para ello complete la carga de la imagen en la ruta `path` en escala de grises y obtenga sus descriptores con los m√©todos apropiados de cv2. Los descriptores obtenidos se a√±adir√°n a nuestra bolsa de palabras (`words`) para despu√©s ser agrupados por palabras que representen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = cv2.SIFT_create()\n",
    "\n",
    "# Extract features\n",
    "print(\"\\nComputing SIFT descriptors...\")\n",
    "time.sleep(0.1)  # Prevents a race condition between tqdm and print statements.\n",
    "descriptors = []\n",
    "for path in tqdm(training_set, unit=\"image\", file=sys.stdout):\n",
    "    # TODO: Load the image from the path in grayscale\n",
    "    image = cv2.imread()\n",
    "    try:\n",
    "        # TODO: Using the cv2 methods get the descriptors for the image\n",
    "        _, descriptor = None\n",
    "    except:\n",
    "        print(f\"WARN: Issue generating descriptor for image {path}\")\n",
    "\n",
    "    if descriptor is not None:\n",
    "        descriptors.append(descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2.3**: Creaci√≥n del vocabulario\n",
    "\n",
    "A√±ada los descriptores a la bolsa (`words`) con el m√©todo add de la misma. Con los descriptores en la bolsa, se va a realizar el agrupamiento de los mismos por palabras para obtener un vocabulario de palabras visuales. Complete la ruta de apertura del archivo y establezca el modo de apertura en escrituria y bineario para poder guardar el vocabulario generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocabulary_size = 100\n",
    "iterations = 20\n",
    "termination_criteria = (cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_EPS, iterations, 1e-6)\n",
    "words = cv2.BOWKMeansTrainer(vocabulary_size, termination_criteria)\n",
    "\n",
    "# TODO: Add all descriptors\n",
    "words.add()\n",
    "\n",
    "time.sleep(0.1)  # Prevents a race condition between tqdm and print statements.\n",
    "print(\"\\nClustering descriptors into\", vocabulary_size, \"words using K-means...\")\n",
    "vocabulary = words.cluster()\n",
    "filename=  \"vocabulary.pickle\"\n",
    "# TODO: Open the file from above in the write and binay mode\n",
    "with open() as f:\n",
    "    pickle.dump([\"SIFT\", vocabulary], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2.4**: Entrenamiento del clasificador\n",
    "\n",
    "Una vez se tiene el vocabulario de palabras visuales, se va a entrenar al classificador. Para ello haga uso de la clase [BoW](./bow.py) y complete los m√©todos que se le indican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BoW()\n",
    "# TODO: Especify the args for the loading method\n",
    "bow.load_vocabulary()\n",
    "\n",
    "image_classifier = ImageClassifier(bow)\n",
    "# TODO: Especify the args for the training method\n",
    "image_classifier.train()\n",
    "classifier = \"classifier\"\n",
    "image_classifier.save(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2.5**: Inferencia en dataset de entrenamiento\n",
    "Con el modelo entrenado se procede a comprobar su desempe√±o con el dataset de entrenamiento. Complete los m√©todos indicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BoW()\n",
    "# TODO: Especify the args for the loading method\n",
    "bow.load_vocabulary()\n",
    "\n",
    "image_classifier = ImageClassifier(bow)\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.load()\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2.6**: Inferencia en dataset de evaluaci√≥n\n",
    "\n",
    "A continuaci√≥n se evalua el desempe√±o del modelo con el dataset de validaci√≥n. Complete los m√©todos indicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BoW()\n",
    "# TODO: Especify the args for the loading method\n",
    "bow.load_vocabulary()\n",
    "\n",
    "image_classifier = ImageClassifier(bow)\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.load()\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pregunta C.2.A**: Cambio de SIFT por Kaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pregunta C.2.B**: ¬øCu√°ntas palabras uso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXTRA - Pregunta C.2.C**: Buscando los mej√≥res par√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Homework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
